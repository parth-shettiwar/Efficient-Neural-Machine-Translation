## CS626 Speech and Natural Language Processing and the Web: Project
**Efficient Neural Machine Translation**  
Anshul Tomar 170070007  
Anwesh Mohanty 170070009  
Parth Shettiwar 170070021   

Code Base:  
The Codes Folder contains 2 .ipynb files.  
1)Ours.ipynb  : Our NMT Model based on RNNsearch with adverserial training and noise added to avoid overfitting.  
2)Transformer.ipynb  : This model is based on **All you need is Attention** paper. We have ran the transformer on Multi30k Dataset.  
To run them, just upload them on a collab notebook, and run all cells.There is no need to download the dataset separetely, it will be automatically imported from torchtext datasets. Also all necessary libraries will be installed  in initial few blocks of notebook.       
Both models have been trained on Multi30k English-German Dataset. The final block computes the Bleu score in each. The training is done for 10 epochs and can be changed in the training bloch by varying the parameter N_EPOCHS.

The basic architecture of code consists of following:    
1)Encoder: Consists of an Embedding layer, bi-directional GRU and a Linear layer     
2)Attention Layer: A feedforward neural network to implement attention  
3)Decoder: Consists of an Embedding layer, GRU and a Linear layer   

Following are some of the attention maps generated by our model:  
![plots](./plots/Attention_map1.png)
![plots](./plots/Attention_map2.png)
![plots](./plots/Attention_map3.png)
![plots](./plots/Attention_map4.png)  

We have conducted various abltaion studies and error analysis to further observe the performance of our model.  
The folder contains plots folder ( which contains all the plots and attention maps), Codes folder (containing the codes), our final Report and Final presentation slides.  





 
